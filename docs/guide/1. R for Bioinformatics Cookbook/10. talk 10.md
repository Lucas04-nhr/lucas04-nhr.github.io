---
title: Data summarisation and statistics
createTime: 2025/12/20 12:49:11
permalink: /docs/r-course/talk10/
---

# Talk 10 {#talk10}

::: note
Reference: [R for Data Science](https://r4ds.had.co.nz)

The book updated to 2^nd^ ed. on July, 2023, here’s a [link](https://r4ds.hadley.nz) to the official website.

::: flex around center
<LinkCard 
    icon="ic:outline-description"
    title="View the original slide"
    href="https://github.com/Lucas04-nhr/R-for-Data-Science/blob/main/talk10.pdf" >
</LinkCard>

<LinkCard 
    icon="ic:round-document-scanner"
    title="View the original Rmd file"
    href="https://github.com/Lucas04-nhr/R-for-Data-Science/blob/main/talk10.Rmd" >
</LinkCard>
:::

## Vector Summarization {#vector-summarization}

### Describe Normal Distribution {#describe-normal-distribution}

You can use `mean` and `sd` to describe normal distributions.

- It's symmetrical.
- Mean and median are the same.
- Most common values are near the mean; less common values are farther from it.
- Standard deviation marks the distance from the mean to the inflection point.

### Functions to generate random normal distrubions {#functions-to-generate-random-normal-distrubions}

```R
qnorm()
pnorm()
dnorm()
```

### Other regular distributions {#other-regular-distributions}

1. Uniform Distributions

```R
dunif(x, min = 0, max = 1, log = FALSE)
punif(q, min = 0, max = 1, lower.tail = TRUE, log.p = FALSE)
qunif(p, min = 0, max = 1, lower.tail = TRUE, log.p = FALSE)
runif(n, min = 0, max = 1)
```

1. Non-parametric Distributions

Here’s an example:

```R
bi =
  c(7, 3, 2, 1, 7,
    3, 4, 5, 7, 6,
    2, 2, 1, 3, 7,
    2, 6, 8, 2, 7,
    2, 2, 1, 3, 5,
    8, 2, 6, 7, 8,
    6, 2, 8, 7, 9,
    2, 7, 5, 1, 8,
    8, 2, 3, 7, 3
   )
ggplot(
  data.frame(dat = bi),
  aes(dat)) +
geom_density()
```

### Quantitative descriptive data {#quantitative-descriptive-data}

- `mean`: aka average, is the sum of all of the numbers in the data set divided by the size of the data set;
- `median`: The median is the value that is in the middle when the numbers in a data set are sorted in increasing order;
- `sd`: standard deviation;
- `var`: measures how far a set of numbers are spread out;
- `range`: range of values.

### Quantitative descriptive function {#quantitative-descriptive-function}

Besides the function with the same name of the data above, `quantile` and `summary` are two quantitative descriptive functions.

::: note
This chapter contains lots of functions and their usage, to know more, you can see them in the official R documentation, I only explane the things above here.
:::

## Statistics {#statistics}

### Parametric tests {#parametric-tests}

#### `t-test` {#t-test}

Detect whether the distribution is consistent with expectations; e.g., whether the number of steps per day for boys is significantly different from 10,000.

The test assesses whether the means of two samples are significantly different from each other, assuming that the samples are normally distributed and have approximately equal variances.

There are different types of t-tests in R, depending on the nature of the comparison:

::: collapse expand

- One-Sample t-test

  Used to determine if the mean of a single sample differs significantly from a known or hypothesized population mean.

  ```R
  # One-sample t-test example
  sample_data = c(17, 21, 19, 23, 20, 18, 22)
  t.test(sample_data, mu = 20)
  ```

  This code performs a one-sample t-test on `sample_data` to test if its mean differs significantly from 20.

- Independent Samples t-test (or Two-Sample t-test)

  Compares the means of two independent groups to determine if they are significantly different from each other.

  ```R
  # Independent samples t-test example
  group1 = c(23, 25, 28, 22, 20)
  group2 = c(18, 21, 24, 19, 17)
  t.test(group1, group2)
  ```

  This code performs an independent samples t-test on `group1` and `group2` to test if their means are significantly different.

- Paired t-test

      Compares the means of two related groups (e.g., before and after measurements) to determine if they are significantly different.

      ```R
      # Paired t-test example
      before = c(32, 28, 30, 29, 31)
      after = c(30, 25, 28, 27, 29)
      t.test(before, after, paired = TRUE)
      ```

      This code performs a paired t-test on `before` and `after` to test if there's a significant difference.

  :::

---

The `t.test()` function in R is used to conduct these t-tests. It returns a test statistic (t-value), degrees of freedom, p-value, and confidence interval, providing insights into whether the difference observed is statistically significant.

It's important to ensure that the assumptions of normality and equal variances are met for reliable results when performing t-tests in R. If the assumptions are violated, alternative tests or data transformations might be more appropriate.

---

#### One-way ANNOVA {#one-way-annova}

In R, the one-way analysis of variance (ANOVA) is used to test for significant differences between the means of three or more independent (unrelated) groups. It assesses whether the means of these groups are significantly different from each other.

The one-way ANOVA assumes that the data meet certain assumptions, including:

- **Normality:** Each group should follow a normal distribution.
- **Homogeneity of variances:** The variances within each group should be approximately equal.
- **Independence:** The observations within each group should be independent of each other.

Here's an example of performing a one-way ANOVA in R:

```R
# Example of one-way ANOVA
group1 = c(15, 20, 25, 30, 35)
group2 = c(10, 18, 25, 32, 40)
group3 = c(12, 22, 28, 32, 38)

# Combining data into a data frame
my_data = data.frame(
  Values = c(group1, group2, group3),
  Group = factor(rep(1:3, each = 5))  # Creating a factor for groups
)

# Performing one-way ANOVA
result_anova = aov(Values ~ Group, data = my_data)
summary(result_anova)
```

==Explanation==

1. The data for three groups (`group1`, `group2`, `group3`) are created.
2. The data are combined into a data frame (`my_data`) where the `Values` column contains the measurements and the `Group` column represents the group labels as a factor.
3. The `aov()` function is used to perform the one-way ANOVA, specifying the formula `Values ~ Group`, indicating that `Values` is the dependent variable and `Group` is the independent variable.
4. `summary(result_anova)` provides the ANOVA table with the F-statistic, degrees of freedom, p-value, and other relevant statistics.

The output from `summary(result_anova)` will include information such as the F-statistic, degrees of freedom, p-value, and within-group variability, allowing you to determine if there are significant differences between the means of the groups.

If the p-value is less than a chosen significance level (commonly 0.05), it suggests that there are significant differences among the means of the groups. Additionally, post-hoc tests like Tukey's HSD test or pairwise t-tests can be performed to identify which specific groups differ significantly from each other after obtaining a significant result in the ANOVA.

---

#### Two-way ANNOVA {#two-way-annova}

A two-way analysis of variance (ANOVA) in R is used to examine the interaction effects between two categorical independent variables (factors) on a continuous dependent variable.

Here's an example:

```R
# Example of two-way ANOVA
# Assume we have a dataset with 'Treatment', 'Gender', and 'Response' variables

# Creating sample data
set.seed(123)
Treatment = rep(c("A", "B", "C"), each = 20)
Gender = rep(c("Male", "Female"), times = 30)
Response = rnorm(60, mean = c(50, 60, 70), sd = 10)

# Combining data into a data frame
my_data = data.frame(Treatment, Gender, Response)

# Performing two-way ANOVA
result_anova = aov(Response ~ Treatment + Gender + Treatment:Gender, data = my_data)
summary(result_anova)
```

==Explanation==

1. Sample data is created with three variables: `Treatment`, `Gender`, and `Response`.
2. The data are combined into a data frame (`my_data`), where `Treatment` and `Gender` are categorical factors, and `Response` is the continuous dependent variable.
3. The `aov()` function performs the two-way ANOVA. The formula `Response ~ Treatment + Gender + Treatment:Gender` specifies the main effects of `Treatment` and `Gender`, as well as their interaction effect.
4. `summary(result_anova)` provides the ANOVA table with F-statistics, degrees of freedom, p-values, and other statistics for each factor and their interaction.

The output from `summary(result_anova)` will include information about the main effects of `Treatment` and `Gender`, as well as the interaction effect between them. It allows you to determine if there are significant effects of each factor independently and whether their interaction significantly influences the `Response` variable.

The interpretation of a two-way ANOVA involves analyzing the p-values associated with each factor and their interaction. Significant p-values indicate that the corresponding factor or interaction has a significant effect on the dependent variable. Additionally, post-hoc tests or further analyses can be conducted to explore specific comparisons between groups or factors after obtaining significant results in the ANOVA.

---

#### Linear Regression {#linear-regression}

Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data. In R, linear regression can be performed using the `lm()` function, which stands for "linear model."

Here's an example:

```R
# Example of simple linear regression
# Suppose we have a dataset with 'x' as the independent variable and 'y' as the dependent variable

# Creating sample data
set.seed(123)
x = 1:50
y = 2 * x + rnorm(50, mean = 0, sd = 5)  # Generating 'y' as a linear function of 'x' with some noise

# Creating a data frame
my_data = data.frame(x, y)

# Performing linear regression
model = lm(y ~ x, data = my_data)
summary(model)
```

==Explanation==

1. Sample data is generated with an independent variable `x` and a dependent variable `y`. In this example, `y` is generated as a linear function of `x` with some added noise using `rnorm()` to simulate real-world variability.
2. The data are combined into a data frame `my_data`.
3. The `lm()` function fits a linear regression model where `y` is the dependent variable and `x` is the independent variable (`y ~ x`). The argument `data = my_data` specifies the data frame containing the variables.
4. `summary(model)` provides a summary of the linear regression model, including coefficients, standard errors, t-values, p-values, R-squared, and other statistics.

Interpreting the output from `summary(model)`:

- The coefficients section shows the estimated coefficients for the intercept and the slope of the regression line (`Intercept` and `x`).
- The p-values associated with the coefficients indicate the significance of each variable in predicting the dependent variable. Lower p-values suggest stronger evidence against the null hypothesis of no effect.
- The R-squared value represents the proportion of variance in the dependent variable explained by the independent variable(s). Higher R-squared values indicate a better fit of the model to the data.

Linear regression in R can also be extended to multiple linear regression by including multiple independent variables in the model (`y ~ x1 + x2 + ...`). Additionally, diagnostic plots and further analyses can be performed to assess model assumptions and goodness of fit.

---

#### Model / Prediction / Coefficients {#model-prediction-coefficients}

Certainly! In linear regression, the model equation is expressed as:

$$
 y = \beta_0 + \beta_1 \cdot x_1 + \beta_2 \cdot x_2 + \ldots + \beta_n \cdot x_n + \varepsilon
$$

Where:
| **Symbol** | **Description** |
|:--------:|:-------------|
| $y$ | The dependent variable |
| $x_1, x_2, \ldots, x_n$ | The independent variables |
| $\beta_0$ | The intercept (constant term) |
| $\beta_1, \beta_2, \ldots, \beta_n$ | The coefficients (slope parameters) that represent the change in $y$ associated with a one-unit change in the corresponding $x$ variable, assuming all other variables remain constant |
| $\varepsilon$ | The error term |

In R, after fitting a linear regression model using `lm()`:

```R
# Assuming 'model' is the linear regression model obtained previously
summary(model)
```

The output from `summary(model)` provides information including:

- **Coefficients:** This section displays the estimated coefficients (`Estimate`) for each independent variable, including the intercept. These coefficients represent the estimated change in the dependent variable for a one-unit change in the corresponding independent variable, holding other variables constant.

- **Residuals:** The residuals represent the differences between the observed and predicted values of the dependent variable. These are used to assess the goodness of fit of the model.

- **R-squared:** Indicates the proportion of variance in the dependent variable explained by the independent variables. Higher values indicate a better fit of the model to the data.

After obtaining the coefficients from the model, predictions can be made using new or existing data:

```R
# Assuming 'new_data' contains the new data for prediction
predicted_values = predict(model, newdata = new_data)
```

Replace `new_data` with the data for which you want to make predictions. The `predict()` function uses the coefficients from the model to generate predicted values of the dependent variable based on the independent variables in the new data.

The coefficients obtained from the linear regression model (`model$coefficients`) represent the slopes of the regression line and the intercept, which are crucial for predicting new values and understanding the relationships between variables in the model.

### Non-parametric Comparison {#non-parametric-comparison}

Non-parametric methods are statistical techniques used when the assumptions of normality, homogeneity of variances, or linearity required by parametric methods are violated or not met by the data. These methods do not rely on specific population distribution assumptions and are useful for analyzing data that might not follow a normal distribution.

Here are some commonly used non-parametric methods for comparison in R:

::: collapse

1.  Mann-Whitney U Test (Wilcoxon Rank-Sum Test)

    Used to compare two independent groups when the assumptions of t-tests are not met.

    ```R
    # Assuming 'group1' and 'group2' are vectors of numeric data
    wilcox.test(group1, group2)
    ```

2.  Kruskal-Wallis Test

    An extension of the Mann-Whitney U test for comparing more than two independent groups.

    ```R
    # Assuming 'group1', 'group2', 'group3' are vectors of numeric data
    kruskal.test(list(group1, group2, group3))
    ```

3.  Wilcoxon Signed-Rank Test

    Used for comparing paired samples or related samples.

    ```R
    # Assuming 'before' and 'after' are vectors of paired numeric data
    wilcox.test(before, after, paired = TRUE)
    ```

4.  Mood's Median Test

    Tests the equality of medians in two or more independent groups.

    ```R
     # Assuming 'group1', 'group2', 'group3' are vectors of numeric data
     median_test = mood.test(group1, group2, group3)
     median_test
    ```

5.  Friedman Test

        An extension of the Wilcoxon Signed-Rank test for comparing more than two paired or related groups.

        ```R
        # Assuming 'group1', 'group2', 'group3' are matrices or data frames of paired numeric data
        friedman.test(group1, group2, group3)
        ```

    :::

These non-parametric tests provide alternatives to traditional parametric tests and are robust against violations of certain assumptions. They are particularly useful when dealing with ordinal or skewed data or when the sample size is small, as they rely on fewer distributional assumptions than parametric tests.
