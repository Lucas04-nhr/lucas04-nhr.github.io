---
title: Linear and nonlinear regression
createTime: 2025/12/20 12:49:11
permalink: /docs/r-course/talk11/
---

# Talk 11 {#talk11}

::: note
Reference: [R for Data Science](https://r4ds.had.co.nz)

The book updated to 2^nd^ ed. on July, 2023, hereâ€™s a [link](https://r4ds.hadley.nz) to the official website.

  ::: flex around center
  <LinkCard 
    icon="ic:outline-description"
    title="View the original slide"
    href="https://github.com/Lucas04-nhr/R-for-Data-Science/blob/main/talk11.pdf" >
  </LinkCard>

  <LinkCard 
    icon="ic:round-document-scanner"
    title="View the original Rmd file"
    href="https://github.com/Lucas04-nhr/R-for-Data-Science/blob/main/talk11.Rmd" >
  </LinkCard>
  :::

## Linear Regression {#linear-regression}

==What is linear regression?==

Linear regression is a method of statistical analysis that utilizes regression analysis in mathematical statistics to determine interdependent quantitative relationships between two or more variables.

- `Y` can be explained by a variable `X`: One-way Linear Regression

- `Y` can be explained by multiple variables such as `X`, `Z`: Multiple Linear Regression

Linear regression in R is typically performed using the `lm()` function, which stands for "linear model." Here's how to fit a linear regression model in R and some useful functions related to linear regression:

### Fitting a Linear Regression Model {#fitting-linear-regression-model}

```R
# Example of fitting a linear regression model
# Assuming 'my_data' is a data frame with 'x' as the independent variable and 'y' as the dependent variable
model = lm(y ~ x, data = my_data)
summary(model)  # Display summary statistics of the model
```

- **`lm()`** Fits a linear regression model. The formula `y ~ x` specifies that `y` is the dependent variable and `x` is the independent variable. `data = my_data` indicates the data frame containing the variables.

- **`summary()`** Displays a summary of the linear regression model, including coefficients, standard errors, t-values, p-values, R-squared, and other statistics.

### Other Useful Functions for Linear Regression Analysis {#other-linear-regression-functions}

::: collapse

1.  `coefficients()`

    Retrieves the coefficients of the linear regression model.

    ```R
    coef = coefficients(model)
    coef
    ```

2.  `predict()`

    Generates predictions using the fitted model.

    ```R
    new_data = data.frame(x = c(10, 20, 30))  # New data for prediction
    predicted_values = predict(model, newdata = new_data)
    predicted_values
    ```

3.  `residuals()`

    Retrieves the residuals (differences between observed and predicted values).

    ```R
    residuals = residuals(model)
    residuals
    ```

4.  `fitted()`

    Retrieves the fitted (predicted) values.

    ```R
    fitted_values = fitted(model)
    fitted_values
    ```

5.  `vcov()`

    Computes the variance-covariance matrix of the coefficients.

    ```R
    var_cov_matrix = vcov(model)
    var_cov_matrix
    ```

6.  `anova()`

    Performs analysis of variance (ANOVA) for the fitted model.

    ```R
    anova_table = anova(model)
    anova_table
    ```

7.  `summary()`

    Provides an overview of the fitted model, including parameter estimates, standard errors, convergence information, and goodness-of-fit statistics.

    ```R
    summary(model)
    ```

8.  `coef()`

    Extracts the estimated coefficients from the fitted model.

    ```R
    coef(model)
    ```

9.  `confint()` (for prediction intervals)

    Computes prediction intervals for the predicted values from the fitted model.

    ```R
    prediction_intervals = predict(model, interval = "prediction")
    prediction_intervals
    ```

10. `deviance()`

    Calculates the deviance of the fitted model, which is a measure of lack of fit.

    ```R
    deviance(model)
    ```

11. `update()`

    Allows for updating or refitting a model with different settings or data.

    ```R
    updated_model = update(model, start = list(a = 2, b = 2, c = 2))
    summary(updated_model)
    ```

12. `AIC()` and `BIC()`

    Compute Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) to assess model quality and compare models.

    ```R
    AIC(model)
    BIC(model)
    ```

13. `plot()` (for diagnostic plots)

        Generates diagnostic plots to assess the adequacy of the model (e.g., residuals vs. fitted values, Q-Q plots).

        ```R
        plot(model)
        ```

    :::

These functions help in obtaining and analyzing various aspects of the linear regression model, such as coefficients, predictions, residuals, variance-covariance matrix, and ANOVA tables, aiding in model interpretation, examining the fitted model's statistics, coefficients, goodness-of-fit measures, prediction intervals, model comparisons, and diagnostics, allowing for a comprehensive analysis of nonlinear regression models in R.

## Nolinear Regression {#nonlinear-regression}

Nonlinear regression is used when the relationship between variables cannot be adequately described by a linear model. In R, fitting nonlinear models involves estimating parameters to describe the nonlinear relationship between variables. Here's an example using the `nls()` function, along with some relevant functions for nonlinear regression analysis:

### Fitting a Nonlinear Regression Model {#fitting-nonlinear-regression-model}

```R
# Example of fitting a nonlinear regression model (assuming a quadratic function)
# Assuming 'my_data' is a data frame with 'x' as the independent variable and 'y' as the dependent variable

# Fitting a quadratic model: y = a * x^2 + b * x + c
model = nls(y ~ a * I(x^2) + b * x + c, data = my_data, start = list(a = 1, b = 1, c = 1))
summary(model)  # Display summary statistics of the model
```

- **`nls()` Function:** Fits a nonlinear regression model. The formula `y ~ a * I(x^2) + b * x + c` specifies a quadratic function. `data = my_data` indicates the data frame containing the variables, and `start = list(a = 1, b = 1, c = 1)` provides initial parameter values.

- **`summary()` Function:** Displays a summary of the nonlinear regression model, including parameter estimates, standard errors, t-values, convergence information, and other statistics.

### Other Useful Functions for Nonlinear Regression Analysis {#other-nonlinear-regression-functions}

::: collapse

1.  `predict()`

    Generates predictions using the fitted nonlinear model.

    ```R
    new_data = data.frame(x = c(10, 20, 30))  # New data for prediction
    predicted_values = predict(model, newdata = new_data)
    predicted_values
    ```

2.  `residuals()`

    Retrieves the residuals (differences between observed and predicted values).

    ```R
    residuals = residuals(model)
    residuals
    ```

3.  `confint()`

    Computes confidence intervals for model parameters.

    ```R
    conf_intervals = confint(model)
    conf_intervals
    ```

4.  `nls.control()`

    Provides control parameters for the `nls()` function, allowing adjustments to the nonlinear fitting process.

    ```R
    control_params = nls.control(maxiter = 100, tol = 1e-6)
    model = nls(y ~ a * I(x^2) + b * x + c, data = my_data, start = list(a = 1, b = 1, c = 1), control = control_params)
    ```

5.  `anova()`

    Performs analysis of variance (ANOVA) for the fitted nonlinear model.

    ```R
    anova_table = anova(model)
    anova_table
    ```

6.  `nlsLM()` (from the `minpack.lm` package)

    An alternative to `nls()` that provides enhanced convergence properties and extended functionality for nonlinear least squares.

    ```R
    library(minpack.lm)
    model_nlsLM = nlsLM(y ~ a * I(x^2) + b * x + c, data = my_data, start = list(a = 1, b = 1, c = 1))
    summary(model_nlsLM)
    ```

7.  `augment()` (from the `broom` package)

    Creates a tidy data frame with additional columns like fitted/predicted values, residuals, and other model information.

    ```R
    library(broom)
    augmented_model = augment(model)
    head(augmented_model)
    ```

8.  `glance()` (from the `broom` package)

    Extracts model-level statistics, providing a summary of the model in a tidy format.

    ```R
    glance_summary = glance(model)
    glance_summary
    ```

9.  `tidy()` (from the `broom` package)

    Extracts the model coefficients and related statistics into a tidy data frame.

    ```R
    tidy_summary = tidy(model)
    tidy_summary
    ```

10. `nlstools::nlstools()`

    Offers diagnostic tools and visualizations for nonlinear regression models, aiding in model evaluation.

    ```R
    library(nlstools)
    model_tools = nlstools(model)
    plot(model_tools)
    ```

11. `nls2()`

        Provides an extended version of `nls()` with multiple start values to improve convergence.

        ```R
        model_nls2 = nls2(y ~ a * I(x^2) + b * x + c, data = my_data, start = list(a = 1, b = 1, c = 1))
        summary(model_nls2)
        ```

    :::

These functions are commonly used for nonlinear regression analysis in R, helping in prediction, residual analysis, confidence interval computation, and controlling the fitting process.

## Modeling and Prediction {#modeling-prediction}

When it comes to modeling and prediction using regression analysis, especially nonlinear regression, understanding the model, making predictions, and assessing the model's performance are crucial. Here's a step-by-step guide on how to approach modeling and prediction:

### Steps {#modeling-prediction-steps}

::: steps

1. **Data Preparation**

   Prepare your dataset with variables for the dependent (response) and independent (predictor) variables.

2. **Fitting the Nonlinear Model**

   Fit the nonlinear regression model using `nls()` or similar functions, specifying the appropriate formula and initial parameter values.

   ```R
   model =
   nls(y ~ a * I(x^2) + b * x + c,
       data = my_data,
       start = list(a = 1, b = 1, c = 1))
   ```

3. **Model Summary and Assessment**

   Use `summary()` and other functions (`glance()`, `tidy()`, etc.) to obtain an overview of the model, including coefficients, goodness-of-fit measures, and diagnostics.

   ```R
   summary(model)
   ```

4. **Prediction with the Fitted Model**

   Generate predictions using the fitted model for new or existing data.

   ```R
   new_data = data.frame(x = c(10, 20, 30))  # New data for prediction
   predicted_values = predict(model, newdata = new_data)
   predicted_values
   ```

5. **Model Evaluation**

   Evaluate the model's performance using various metrics (e.g., residuals, R-squared, RMSE) and diagnostic plots (e.g., residuals vs. fitted values, Q-Q plots) to assess how well the model fits the data.

6. **Adjustment and Refinement**

   Depending on the evaluation results, consider refining the model by adjusting parameters, exploring different models, or including/excluding variables to improve performance.

7. **Prediction Intervals and Uncertainty**

   Compute prediction intervals using `predict()` with `interval = "prediction"` to quantify the uncertainty around predictions.

8. **Model Comparison (if applicable)**

   Compare multiple models using metrics like AIC, BIC, or likelihood ratio tests to select the most appropriate model.

:::

By following these steps, you'll be able to build, evaluate, and utilize nonlinear regression models for prediction in R effectively. Remember, interpreting the model's results, assessing its assumptions, and validating predictions are crucial aspects of regression analysis.

## **K-fold** & **X times** cross-validation {#kfold-xtimes-crossvalidation}

Both K-fold cross-validation and X times cross-validation are techniques used to assess the performance and generalization capability of machine learning models, including regression models, by partitioning the data into subsets for training and validation.

### K-fold Cross-Validation {#kfold-crossvalidation}

K-fold cross-validation involves splitting the dataset into K equally sized folds. The model is trained K times, each time using K-1 folds for training and the remaining fold for validation. This process ensures that each data point is used for both training and validation.

#### Steps for K-fold Cross-Validation {#steps-kfold-crossvalidation}

::: steps

1.  **Partition Data**

    Split the dataset into K equally sized folds.

2.  **Model Training**

    Train the model K times, each time using K-1 folds as training data.

3.  **Validation**

    Validate the model's performance on the remaining fold (not used in training) and calculate evaluation metrics.

4.  **Average Metrics**

        Average the evaluation metrics across the K iterations to obtain an overall assessment of the model's performance.

    :::

### X times Cross-Validation {#xtimes-crossvalidation}

X times cross-validation, also known as repeated K-fold cross-validation, is similar to K-fold cross-validation, but the process is repeated X times. It repeatedly creates random partitions of the data into K folds, trains the model, and evaluates its performance. This method provides more robust estimates of model performance by averaging results over multiple runs.

#### Steps for X times Cross-Validation {#steps-xtimes-crossvalidation}

::: steps

1.  **Partition Data Repeatedly**

    Randomly split the dataset into K folds, X times.

2.  **Model Training**

    Train the model for each iteration, using K-1 folds for training.

3.  **Validation**

    Validate the model's performance on the remaining fold and calculate evaluation metrics.

4.  **Average Metrics**

        Average the evaluation metrics across the X iterations to obtain more stable and reliable estimates of model performance.

    :::

### Implementation in R {#implementation-r}

In R, you can perform K-fold and X times cross-validation using functions from various packages like `caret`, `rsample`, or `crossval`. For example, using `caret`:

#### K-fold Cross-Validation in R with `caret` {#kfold-in-r-caret}

```R
library(caret)
# Define a train control using k-fold cross-validation
train_control =
  trainControl(method = "cv", number = K)
# Specify K for the number of folds

# Train the model using k-fold cross-validation
model =
  train(
    formula,
    data = my_data,
    method = "lm",
    trControl = train_control
  )
```

#### X times Cross-Validation in R with `caret` {#xtimes-in-r-caret}

```R
library(caret)
# Define a train control using repeated k-fold cross-validation
train_control =
trainControl(
  method = "repeatedcv",
  number = K, repeats = X)
# Specify K for folds and X for repeats

# Train the model using repeated k-fold cross-validation
model =
  train(
    formula,
    data = my_data,
    method = "lm",
    trControl = train_control
  )
```

::: note
Replace `formula` and `my_data` with the appropriate regression formula and your dataset. Adjust the parameters `K` and `X` according to your preferences for the number of folds and repetitions.
:::

These cross-validation techniques aid in estimating the model's performance and help in assessing how well the model generalizes to unseen data, thus providing insights into its robustness and reliability. Adjusting these techniques can improve the evaluation of regression models in terms of accuracy and stability.

## External Validation {#external-validation}

External validation refers to the evaluation of a predictive model's performance using an independent dataset that was not used in the model development process. It serves as an essential step to assess how well a model generalizes to new, unseen data from a different source or time period, verifying its reliability and robustness in real-world applications.

### Steps for External Validation {#steps-external-validation}

::: steps

1.  **Obtain an Independent Dataset**

    Acquire a separate dataset that is distinct from the one used for model training and validation. This dataset should ideally represent the same problem or domain but come from a different source, time frame, or population.

2.  **Preprocess Data**

    Preprocess the independent dataset similarly to the training dataset (e.g., handling missing values, encoding categorical variables, scaling features) to ensure compatibility.

3.  **Apply Trained Model**

    Use the model trained on the original dataset to make predictions on the independent dataset.

4.  **Evaluate Model Performance**

    Assess the model's performance on the independent dataset using relevant evaluation metrics (e.g., accuracy, RMSE, precision, recall) and compare these metrics to the performance achieved on the training/validation dataset.

5.  **Analyze Results**

        Analyze the performance metrics obtained from the independent dataset to determine if the model maintains its predictive capability and generalizability. A well-performing model on the independent dataset suggests good generalization and reliability.

    :::

### Importance of External Validation {#importance-external-validation}

- **Generalization Assessment:** Validates whether the model's performance seen in training/validation data extends to new, unseen data.
- **Bias and Overfitting Detection:** Identifies if the model is overfitted to the training data or exhibits biases that could limit its real-world applicability.
- **Real-world Applicability:** Confirms the model's utility in practical scenarios and different environments.
- **Trustworthiness and Reliability:** Provides stakeholders with confidence in the model's predictions and results.

### Implementation in R {#implementation-r-external-validation}

In R, the process involves loading the trained model and applying it to the independent dataset for prediction. Use appropriate evaluation functions (`predict()`, evaluation metrics) to assess the model's performance on the external dataset.

```R
# Load the trained model (replace 'model' with your trained model)
load("trained_model.RData")

# Load and preprocess the independent dataset
# (replace 'independent_data.csv' with your dataset)
independent_data = read.csv("independent_data.csv")
# Perform similar preprocessing steps as used for the training data

# Apply the trained model to the independent dataset for prediction
predicted_values = predict(model, newdata = independent_data)

# Evaluate model performance on the independent dataset
# Use appropriate evaluation metrics
# (e.g., RMSE, accuracy)
# and compare with training/validation results
```

::: note
Replace the file paths, data loading, and evaluation steps with your specific dataset and evaluation procedures. Ensure the compatibility of the independent dataset with the preprocessing steps applied to the original dataset for accurate evaluation.
:::
