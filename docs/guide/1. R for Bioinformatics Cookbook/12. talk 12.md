---
title: Machine learning basics
createTime: 2025/12/20 13:44:19
permalink: /docs/r-course/talk12/
---

# Talk 12 {#talk12}

::: note
Reference: [R for Data Science](https://r4ds.had.co.nz)

The book updated to 2^nd^ ed. on July, 2023, hereâ€™s a [link](https://r4ds.hadley.nz) to the official website.

  ::: flex around center
  <LinkCard 
    icon="ic:outline-description"
    title="View the original slide"
    href="https://github.com/Lucas04-nhr/R-for-Data-Science/blob/main/talk12.pdf" >
  </LinkCard>

  <LinkCard 
    icon="ic:round-document-scanner"
    title="View the original Rmd file"
    href="https://github.com/Lucas04-nhr/R-for-Data-Science/blob/main/talk12.Rmd" >
  </LinkCard>
  :::

## Machine Learning Algorithms Generalization {#machine-learning-algorithms-generalization}

Machine learning (ML) is a subset of artificial intelligence (AI) that focuses on developing algorithms and statistical models that enable computers to perform specific tasks without explicit instructions. Instead, these algorithms learn patterns from data and make predictions or decisions based on that learning.

Machine learning can be categorized as follows:

- Regression Algorithms
- Example-based algorithms
- Decision Tree Learning
- Bayesian methods
- Kernel-based algorithms
- Clustering algorithms
- Dimensionality reduction algorithms
- Association rule learning
- Integration algorithms
- Artificial Neural Networks

For more information, you can read related articles.

## Random Forest {#random-forest}

Certainly! Random Forest is a versatile and powerful machine learning algorithm used for both classification and regression tasks. It operates by building multiple decision trees during training and then merging their predictions to improve accuracy and robustness. Here's a guide on implementing Random Forest in R:

### Steps to Implement Random Forest in R {#steps-to-implement-random-forest-in-r}

::: steps

1.  **Load Required Library**

    Start by loading the necessary library (`randomForest`) if not already installed.

    ```R
    install.packages("randomForest")  # Install package if not installed
    library(randomForest)
    ```

2.  **Prepare Data**

    Load your dataset into R and perform necessary preprocessing steps such as handling missing values, encoding categorical variables, and splitting the data into training and testing sets.

    ```R
    # Example: Assuming 'my_data' is your dataset
    # Split data into features (X) and target variable (Y)
    X = my_data[, -target_column_index]  # Features
    Y = my_data[, target_column_index]   # Target variable
    ```

3.  **Train the Random Forest Model**

    Use the `randomForest()` function to train the model by specifying the formula and training data.

    ```R
    # Example: Training a Random Forest model for regression
    model = randomForest(Y ~ ., data = my_data)
    # For classification: model = randomForest(factor(Y) ~ ., data = my_data)
    ```

4.  **Model Tuning (Optional)**

    Adjust hyperparameters such as the number of trees (`ntree`), maximum depth (`max_depth`), and others to optimize model performance.

    ```R
    # Example: Setting number of trees and maximum depth
    model = randomForest(Y ~ ., data = my_data, ntree = 100, max_depth = 10)
    ```

5.  **Make Predictions**

    Use the trained model to make predictions on new or test data.

    ```R
    # Example: Making predictions on test data
    predicted_values = predict(model, newdata = test_data)
    ```

6.  **Model Evaluation**

        Evaluate the model's performance using appropriate metrics (e.g., RMSE for regression, accuracy, confusion matrix for classification) on test/validation data.

        ```R
        # Example: Evaluate model performance (for regression)
        error = sqrt(mean((predicted_values - true_values)^2))  # Calculate RMSE
        ```

    :::

### Example {#example-random-forest}

Here's an example using a built-in dataset (`mtcars`) in R for regression:

```R
library(randomForest)

# Load dataset
data(mtcars)

# Split data into features and target variable
X = mtcars[, -1]  # Exclude the first column (target variable)
Y = mtcars[, 1]   # First column (target variable)

# Train the Random Forest model
model = randomForest(Y ~ ., data = mtcars)

# Make predictions on the same dataset (for demonstration)
predicted_values = predict(model, newdata = mtcars)

# Evaluate model performance (RMSE)
error = sqrt(mean((predicted_values - mtcars$mpg)^2))
```

::: info
Replace `my_data`, `test_data`, and the respective column names with your specific dataset and target variable. Adjust hyperparameters based on your problem and dataset characteristics for better model performance. Additionally, use appropriate evaluation metrics for assessing model accuracy and performance.
:::

## Feature Selection {#feature-selection}

Feature selection is a crucial step in machine learning aimed at choosing the most relevant and informative features to improve model performance, reduce computational complexity, and mitigate overfitting. In R, various methods can help identify important features.

### Feature Selection Techniques in R {#feature-selection-techniques-in-r}

::: steps

1.  **Correlation Analysis**

    Calculate correlation coefficients between features and the target variable or among features themselves to identify highly correlated features. Remove redundant or highly correlated features.

    ```R
    # Example: Using cor() for correlation analysis
    correlation_matrix = cor(my_data)
    ```

2.  **Filter Methods**

    Use statistical measures (e.g., chi-squared test, mutual information) to rank and select features based on their individual relevance to the target variable.

    ```R
    # Example: Using chi-squared test for feature selection
    library(caret)
    feature_selection = nearZeroVar(my_data)
    ```

3.  **Wrapper Methods**

    Evaluate subsets of features by training models iteratively and selecting the subset that results in the best model performance.

    ```R
    # Example: Using recursive feature elimination (RFE)
    library(caret)
    control = rfeControl(functions = rfFuncs, method = "cv", number = 10)
    feature_selection = rfe(X, Y, sizes = c(1:10), rfeControl = control)
    ```

4.  **Embedded Methods**

    Feature selection during model training where algorithms inherently perform feature selection (e.g., LASSO, Random Forest, Gradient Boosting).

    ```R
    # Example: Using Random Forest for feature selection
    library(randomForest)
    model = randomForest(Y ~ ., data = my_data)
    importance = importance(model)
    ```

5.  **Dimensionality Reduction Techniques**

        Techniques like Principal Component Analysis (PCA) or t-Distributed Stochastic Neighbor Embedding (t-SNE) reduce feature space by creating new variables that capture most of the original information.

        ```R
        # Example: PCA for feature reduction
        pca_result = prcomp(my_data, scale. = TRUE)
        ```

    :::

### Implementation Considerations {#implementation-considerations}

- **Domain Knowledge:** Utilize domain expertise to guide feature selection, understanding the relevance of features in the context of the problem.
- **Cross-Validation:** Perform feature selection within cross-validation loops to prevent overfitting and ensure generalizability.
- **Evaluate Model Performance:** Assess the impact of feature selection on model performance using appropriate evaluation metrics.

Choose feature selection methods based on the dataset's characteristics, problem complexity, and computational resources. A combination of multiple techniques often yields the best results, as different methods capture different aspects of feature importance. Experiment and iterate to find the most suitable features for your machine learning models.
